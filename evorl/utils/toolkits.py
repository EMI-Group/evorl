import jax
import jax.numpy as jnp

import chex

from typing import Tuple

from evorl.types import SampleBatch


def compute_episode_timesteps(
    dones: chex.Array,  # [T, B]
) -> chex.Array:
    """
        dones: should be collected from episodic trajectory
    """
    # [B]
    timesteps = (1-dones).sum(axis=0)+1
    return timesteps


def compute_discount_return(
        rewards: chex.Array,  # [T, B]
        dones: chex.Array,  # [T, B]
        discount: float = 1.0) -> chex.Array:

    def _compute_discount_return(discount_return, x_t):
        # G_t := r_t + Î³ * G_{t+1}
        reward_t, discount_t = x_t
        discount_return = reward_t + discount_return * discount_t

        return discount_return, None

    # [#envs]
    discount_return = jnp.zeros_like(rewards[0])

    discount_return, _ = jax.lax.scan(
        _compute_discount_return,
        discount_return,
        (rewards, (1 - dones)*discount),
        reverse=True,
        unroll=16
    )

    return discount_return  # [B]


def compute_gae(rewards: jax.Array,  # [T, B]
                values: jax.Array,  # [T+1, B]
                dones: jax.Array,  # [T, B]
                gae_lambda: float = 1.0,
                discount: float = 0.99) -> Tuple[jax.Array, jax.Array]:
    """
    Calculates the Generalized Advantage Estimation (GAE).

    Args:
        rewards: A float32 tensor of shape [T, B] containing rewards generated by
          following the behaviour policy.
        values: A float32 tensor of shape [T+1, B] with the value function estimates
          wrt. the target policy. values[T] is the bootstrap_value
        dones: A float32 tensor of shape [T, B] with truncation signal.
        gae_lambda: Mix between 1-step (gae_lambda=0) and n-step (gae_lambda=1). 
        discount: TD discount.

    Returns:
        A float32 tensor of shape [T, B]. Can be used as target to
          train a baseline (V(x_t) - vs_t)^2.
        A float32 tensor of shape [T, B] of advantages.
    """
    rewards_shape = rewards.shape
    chex.assert_shape(values, (rewards_shape[0]+1, *rewards_shape[1:]))

    deltas = rewards + discount * (1 - dones) * values[1:] - values[:-1]

    last_gae = jnp.zeros_like(values[0])

    def _compute_gae(gae_t_plus_1, x_t):
        delta_t, factor_t = x_t
        gae_t = delta_t + factor_t * gae_t_plus_1

        return gae_t, gae_t

    _, advantages = jax.lax.scan(
        _compute_gae,
        last_gae,
        (deltas, discount*gae_lambda*(1-dones)),
        reverse=True,
        unroll=16
    )

    lambda_retruns = advantages + values[:-1]

    return jax.lax.stop_gradient(lambda_retruns), jax.lax.stop_gradient(advantages)


def shuffle_sample_batch(sample_batch: SampleBatch, key: chex.PRNGKey):
    return jax.tree_util.tree_map(
        lambda x: jax.random.permutation(key, x),
        sample_batch
    )
