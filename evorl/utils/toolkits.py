import jax
import jax.numpy as jnp

import chex

from typing import Tuple

from evorl.types import SampleBatch


def compute_episode_length(
    dones: chex.Array,  # [T, B]
) -> chex.Array:
    """
        dones: should be collected from episodic trajectory
    """
    # [B]
    episode_lengths = (1-dones).sum(axis=0)+1
    return episode_lengths


def compute_discount_return(
        rewards: chex.Array,  # [T, B]
        dones: chex.Array,  # [T, B]
        discount: float = 1.0) -> chex.Array:
    """
        For episodic trajectory
    """
    def _compute_discount_return(discount_return, x_t):
        # G_t := r_t + γ * G_{t+1}
        reward_t, discount_t = x_t
        discount_return = reward_t + discount_return * discount_t

        return discount_return, None

    # [#envs]
    discount_return = jnp.zeros_like(rewards[0])

    discount_return, _ = jax.lax.scan(
        _compute_discount_return,
        discount_return,
        (rewards, (1 - dones)*discount),
        reverse=True,
        unroll=16
    )

    return discount_return  # [B]

def compute_discount_return_mod(
        rewards: chex.Array,  # [T, B]
        dones: chex.Array,  # [T, B]
        prev_dones: chex.Array,  # [B]
        discount: float = 1.0) -> chex.Array:
    """
        for autoreset envs trajectory
    """

    def _compute_discount_return(carry, x_t):
        # G_t := r_t + γ * G_{t+1}

        discount_return_sum, discount_return = carry
        reward_t, dones_t = x_t
        discount_return_sum += discount_return*dones_t
        discount_return = reward_t + discount_return * (1-dones_t)*discount

        return (discount_return_sum, discount_return), None

    # [#envs]
    discount_return = jnp.zeros_like(rewards[0])

    discount_return_sum, discount_return, _ = jax.lax.scan(
        _compute_discount_return,
        discount_return,
        (rewards, dones),
        reverse=True,
        unroll=16
    )

    # case: add first episode's discount_return if it is complete:
    # i.e. prev_dones = 1
    discount_return_sum += discount_return*prev_dones

    discount_return = discount_return_sum / dones.sum(axis=0)

    return discount_return  # [B]


def compute_gae(rewards: jax.Array,  # [T, B]
                values: jax.Array,  # [T+1, B]
                dones: jax.Array,  # [T, B]
                gae_lambda: float = 1.0,
                discount: float = 0.99) -> Tuple[jax.Array, jax.Array]:
    """
    Calculates the Generalized Advantage Estimation (GAE).

    Args:
        rewards: A float32 tensor of shape [T, B] containing rewards generated by
          following the behaviour policy.
        values: A float32 tensor of shape [T+1, B] with the value function estimates
          wrt. the target policy. values[T] is the bootstrap_value
        dones: A float32 tensor of shape [T, B] with truncation signal.
        gae_lambda: Mix between 1-step (gae_lambda=0) and n-step (gae_lambda=1). 
        discount: TD discount.

    Returns:
        A float32 tensor of shape [T, B]. Can be used as target to
          train a baseline (V(x_t) - vs_t)^2.
        A float32 tensor of shape [T, B] of advantages.
    """
    rewards_shape = rewards.shape
    chex.assert_shape(values, (rewards_shape[0]+1, *rewards_shape[1:]))

    deltas = rewards + discount * (1 - dones) * values[1:] - values[:-1]

    last_gae = jnp.zeros_like(values[0])

    def _compute_gae(gae_t_plus_1, x_t):
        delta_t, factor_t = x_t
        gae_t = delta_t + factor_t * gae_t_plus_1

        return gae_t, gae_t

    _, advantages = jax.lax.scan(
        _compute_gae,
        last_gae,
        (deltas, discount*gae_lambda*(1-dones)),
        reverse=True,
        unroll=16
    )

    lambda_retruns = advantages + values[:-1]

    return jax.lax.stop_gradient(lambda_retruns), jax.lax.stop_gradient(advantages)


def shuffle_sample_batch(sample_batch: SampleBatch, key: chex.PRNGKey):
    return jax.tree_util.tree_map(
        lambda x: jax.random.permutation(key, x),
        sample_batch
    )


def soft_target_update(target_params, source_params, tau: float):
    """
    Perform soft update of target network

    Args:
        target_params: target network parameters
        source_params: source network parameters
        tau: interpolation factor

    Returns:
        updated target network parameters
    """

    return jax.tree_util.tree_map(
        lambda target, source: tau * source + (1 - tau) * target,
        target_params, source_params)


def flatten_rollout_trajectory(trajectory: SampleBatch):
    """
        Flatten the trajectory from [T, B, ...] to [T*B, ...]
    """
    return jax.tree_util.tree_map(
        lambda x: x.reshape(-1, *x.shape[2:]),
        trajectory
    )


def average_episode_discount_return(
    episode_discount_return: jax.Array,  # [T,B]
    dones: jax.Array  # [T,B]
):
    """
        For autoreset envs trajectory.
    """
    # [B]
    cnt = dones.sum(axis=0)
    episode_discount_return_mean = (
        (episode_discount_return * dones).sum(axis=0) / cnt
    )

    return jnp.where(
        jnp.isclose(cnt, 0),
        jnp.zeros_like(episode_discount_return_mean),
        episode_discount_return_mean
    )
