# @package _global_

workflow_cls: evorl.agents.td3.TD3Workflow

num_envs: 10
normalize_obs: false
rollout_length: 1
discount: 0.99
exploration_epsilon: 0.1
total_timesteps: 1000000 # 1000000

num_eval_envs: 10
eval_interval: 100 
log_interval: 100
eval_episodes: 10 # should be divided by num_eval_envs, keep the evaluation's trajatory numbers
actor_update_interval: 2
tau: 0.005
learning_starts: 25000 # steps before training starts 25000
check_load: false
load: false
load_path: ./outputs/train/2024-04-12_09-27-58

optimizer:
  lr: 3e-4
  grad_clip_norm: 10.0 # set 0 or none to turn-off

replay_buffer:
  capacity: 1000000
  min_size: 300
  sample_batch_size: 256

agent_network:
  n_critics: 2
  policy_noise: 0.2
  policy_noise_clip: 0.5
  continuous_action: true
  critic_hidden_layer_sizes: [256, 256]
  actor_hidden_layer_sizes: [256, 256]

checkpoint:
  enable: false
  save_interval_steps: 100000
  max_to_keep: 10