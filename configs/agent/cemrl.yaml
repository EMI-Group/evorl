# @package _global_

workflow_cls: evorl.algorithms.erl.cemrl.CEMRLWorkflow

num_envs: 1
normalize_obs: false
discount: 0.99
total_episodes: 100000

# random_timesteps: 25600 # steps filled into the replay buffer
# learning_start_timesteps: 25600 # steps before training starts
random_timesteps: 0 # steps filled into the replay buffer
learning_start_timesteps: 0 # steps before training starts
batch_size: 256
replay_buffer_capacity: 1000000

tau: 0.005 # soft update rate
exploration_epsilon: 0.1
policy_noise: 0.2
clip_policy_noise: 0.5
actor_update_interval: 2

# cem hyperparams:
warmup_iters: 100 # steps that only learn by CEM

pop_size: 32
num_learning_offspring: 16 # number of offspring to learn from RL(TD3)

num_elites: 8
diagonal_variance:
  init: 0.01
  final: 1e-5
  decay: 0.001 # Polyak averaging step-size, 0.001 is suitable for 1000 iters
weighted_update: true
rank_weight_shift: 1.0 # CEM-RL use 1.0; CMA-ES use 0.5; not significant diff when num_elites is large
mirror_sampling: false

fitness_with_exploration: false
episodes_for_fitness: 1 # must be devided by num_envs
# cemrl_update_interval: 8192 # unit: num of RL updates per CEMRL iteration
num_updates_per_iter: 512 # num of RL updates per iter

loss_weights:
  actor_loss: 1.0
  critic_loss: 0.5

optimizer:
  lr: 0.0003
  grad_clip_norm: 0 # set 0 or none to turn-off

agent_network:
  static_layer_norm: true
  critic_hidden_layer_sizes: [256, 256]
  actor_hidden_layer_sizes: [256, 256]


num_eval_envs: 128
eval_episodes: 128 # should be divided by num_eval_envs
eval_interval: 1

save_replay_buffer: true
