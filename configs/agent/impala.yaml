# @package _global_

workflow_cls: evorl.agents.impala.IMPALAWorkflow

num_envs: 256

normalize_obs: false
rollout_length: 400 # train_batch_size = rollout_length * num_envs = 2048
discount: 0.99
vtrace_lambda: 1.0
clip_rho_threshold: 1.0
clip_c_threshold: 1.0
clip_pg_rho_threshold: 1.0

adv_mode: official
pg_loss_mode: a2c
clip_ppo_epsilon: 0.2

minibatch_size: 128 # unit: trajactories

total_timesteps: 10000000

num_eval_envs: 8
eval_interval: 5
eval_episodes: 16 # should be divided by num_eval_envs

optimizer:
  lr: 0.0003
  grad_clip_norm: 10.0 # set 0 or none to turn-off
  loss_weights:
    actor_loss: 1.0
    critic_loss: 0.5
    actor_entropy_loss: -0.01

agent_network:
  continuous_action: true
  actor_hidden_layer_sizes: [256, 256]
  critic_hidden_layer_sizes: [256, 256]
