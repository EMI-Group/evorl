# @package _global_

workflow_cls: evorl.agents.sac.SACWorkflow

num_envs: 32

normalize_obs: false
rollout_length: 1
discount: 0.99
total_timesteps: 1000000
fold_iters: 64

num_eval_envs: 16
eval_episodes: 16 # should be divided by num_eval_envs
eval_interval: 64

random_timesteps: 12800 # steps filled into the replay buffer
learning_start_timesteps: 12800 # steps before training starts
batch_size: 256
replay_buffer_capacity: 1000000

reward_scale: 10.0
tau: 0.005
adaptive_alpha: false
alpha: 0.2 # used as initial alpha value if adaptive_alpha is true
actor_update_interval: 2
num_updates_per_iter: 32 # #updates per rollout, usually use: num_envs * rollout_length // actor_update_interval

loss_weights:
  actor_loss: 1.0
  critic_loss: 1.0
  alpha_loss: 1.0

optimizer:
  lr: 0.0003
  grad_clip_norm: 10.0 # set 0 or null to turn-off

agent_network:
  continuous_action: true
  critic_hidden_layer_sizes: [256, 256]
  actor_hidden_layer_sizes: [256, 256]

save_replay_buffer: true